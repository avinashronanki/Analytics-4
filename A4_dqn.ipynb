{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4-dqn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMih4YfF5BmoJl8RTQlkrbx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avinashronanki/Analytics-4/blob/main/A4_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f77ZUUrx--v7"
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVDchr5u8a3I"
      },
      "source": [
        "import torch\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81Ol0jTB8UgK"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from models import DQN\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "from itertools import count\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "# Constants for training\n",
        "use_cuda = torch.cuda.is_available()\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "\n",
        "# Preprocessing\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def downsample(img):\n",
        "    return img[::2, ::2]\n",
        "\n",
        "\n",
        "def preprocess(img):\n",
        "    img = downsample(img)\n",
        "    return img.astype(np.float)\n",
        "\n",
        "\n",
        "def choose_best_action(model, state):\n",
        "    state = Variable(torch.FloatTensor(state))\n",
        "    if use_cuda:\n",
        "        state = state.cuda()\n",
        "        model = model.cuda()\n",
        "    state = state.unsqueeze(0)\n",
        "    state = torch.transpose(state, 1, 3)\n",
        "    state = torch.transpose(state, 2, 3)\n",
        "    Q_values = model(state)\n",
        "    value, indice = Q_values.max(1)\n",
        "    action = indice.data[0]\n",
        "    return action\n",
        "\n",
        "\n",
        "def get_epsilon_iteration(steps_done):\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                              math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    return eps_threshold\n",
        "\n",
        "\n",
        "def fit_batch(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, criterion,\n",
        "              iteration, learning_rate, use_polyak_averaging=True, polyak_constant=0.001):\n",
        "\n",
        "    # Step 1: Sample mini batch from B uniformly\n",
        "    if buffer.get_buffer_size() < batch_size:\n",
        "        return 0, 0\n",
        "    batch = buffer.sample_batch(batch_size)\n",
        "    states = []\n",
        "    new_states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    for k in batch:\n",
        "        state, action, new_state, reward = k\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        new_states.append(new_state)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    states = torch.FloatTensor(states)\n",
        "    states = torch.transpose(states, 1, 3)\n",
        "    states = torch.transpose(states, 2, 3)\n",
        "    states = Variable(states)\n",
        "    new_states = torch.FloatTensor(new_states)\n",
        "    new_states = torch.transpose(new_states, 1, 3)\n",
        "    new_states = torch.transpose(new_states, 2, 3)\n",
        "    new_states = Variable(new_states)\n",
        "\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    rewards = Variable(rewards)\n",
        "\n",
        "    actions = torch.LongTensor(actions)\n",
        "    actions = actions.view(-1, 1)\n",
        "    actions = Variable(actions)\n",
        "\n",
        "    if use_cuda:\n",
        "        states = states.cuda()\n",
        "        actions = actions.cuda()\n",
        "        rewards = rewards.cuda()\n",
        "        new_states = new_states.cuda()\n",
        "        target_dqn_model = target_dqn_model.cuda()\n",
        "        dqn_model = dqn_model.cuda()\n",
        "\n",
        "    for p in target_dqn_model.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    # Step 2: Compute the target values using the target network\n",
        "    Q_values = target_dqn_model(new_states)\n",
        "    next_Q_values, indice = Q_values.max(1)\n",
        "    y = rewards + gamma*next_Q_values\n",
        "    y = y.detach()\n",
        "\n",
        "    model_parameters = dqn_model.parameters()\n",
        "\n",
        "    optimizer = optim.Adam(model_parameters, lr=learning_rate)\n",
        "\n",
        "    # Zero the optimizer gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = dqn_model(states)\n",
        "    outputs = outputs.gather(1, actions)\n",
        "    loss = criterion(outputs, y)\n",
        "    loss.backward()\n",
        "    # Gradient clipping\n",
        "    for p in dqn_model.parameters():\n",
        "        p.grad.data.clamp(-1,1)\n",
        "    optimizer.step()\n",
        "    # Stabilizes training as proposed in the DDPG paper\n",
        "    if use_polyak_averaging:\n",
        "        t = polyak_constant\n",
        "        target_dqn_model.conv1.weight.data = t*(dqn_model.conv1.weight.data) + \\\n",
        "                                             (1-t)*(target_dqn_model.conv1.weight.data)\n",
        "        target_dqn_model.bn1.weight.data = t * (dqn_model.bn1.weight.data) + \\\n",
        "                                             (1 - t) * (target_dqn_model.bn1.weight.data)\n",
        "        target_dqn_model.conv2.weight.data = t * (dqn_model.conv2.weight.data) + \\\n",
        "                                             (1 - t) * (target_dqn_model.conv2.weight.data)\n",
        "        target_dqn_model.bn2.weight.data = t * (dqn_model.bn2.weight.data) + \\\n",
        "                                             (1 - t) * (target_dqn_model.bn2.weight.data)\n",
        "        target_dqn_model.conv3.weight.data = t * (dqn_model.conv3.weight.data) + \\\n",
        "                                             (1 - t) * (target_dqn_model.conv3.weight.data)\n",
        "        target_dqn_model.bn3.weight.data = t * (dqn_model.bn3.weight.data) + \\\n",
        "                                             (1 - t) * (target_dqn_model.bn3.weight.data)\n",
        "        target_dqn_model.fully_connected_layer.weight.data = t * (dqn_model.fully_connected_layer.weight.data) + \\\n",
        "                                             (1 - t) * (target_dqn_model.fully_connected_layer.weight.data)\n",
        "        target_dqn_model.output_layer.weight.data = t * (dqn_model.output_layer.weight.data) + \\\n",
        "                                             (1 - t) * (target_dqn_model.output_layer.weight.data)\n",
        "    else:\n",
        "        if n == iteration:\n",
        "            target_dqn_model.load_state_dict(dqn_model.state_dict())\n",
        "\n",
        "    return loss, torch.sum(rewards)\n",
        "\n",
        "\n",
        "def train(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, num_epochs, criterion, learning_rate,\n",
        "          use_double_q_learning = False):\n",
        "    for iteration in range(num_epochs):\n",
        "        print(\"Epoch \", iteration)\n",
        "        state = env.reset()\n",
        "        state = preprocess(state)\n",
        "        loss = 0\n",
        "        re = 0\n",
        "        # Populate the buffer\n",
        "        for t in count():\n",
        "            global steps_done\n",
        "            epsilon = get_epsilon_iteration(steps_done)\n",
        "            steps_done +=1\n",
        "            # Choose a random action\n",
        "            if random.random() < epsilon:\n",
        "                action = env.action_space.sample()\n",
        "                new_state, reward, done, info = env.step(action)\n",
        "            else:\n",
        "                if use_double_q_learning:\n",
        "                    action = choose_best_action(dqn_model, state)\n",
        "                else:\n",
        "                    action = choose_best_action(target_dqn_model, state)\n",
        "                new_state, reward, done, info = env.step(action)\n",
        "\n",
        "            new_state = preprocess(new_state)\n",
        "            buffer.add((state, action, new_state, reward))\n",
        "            state = new_state\n",
        "            # Fit the model on a batch of data\n",
        "            loss_n, r = fit_batch(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, criterion, iteration, learning_rate)\n",
        "            #print(loss)\n",
        "            loss += loss_n\n",
        "            re += r\n",
        "            if done:\n",
        "                break\n",
        "        print(\"Loss for episode\", iteration, \" is \", loss.data/t)\n",
        "        print(\"Reward for episode\", iteration, \" is \", re)\n",
        "\n",
        "    return target_dqn_model, dqn_model\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make('BreakoutDeterministic-v4')\n",
        "    input_shape = env.observation_space.shape\n",
        "    img_height, img_width, img_channels = input_shape\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    dqn_model = DQN.ActionPredictionNetwork(num_conv_layers=16, input_channels=img_channels,\n",
        "                                            output_q_value=num_actions, pool_kernel_size=3,\n",
        "                                            kernel_size=3, dense_layer_features=256,\n",
        "                                            IM_HEIGHT=img_height//2, IM_WIDTH=img_width//2)\n",
        "\n",
        "    target_dqn_model = DQN.ActionPredictionNetwork(num_conv_layers=16, input_channels=img_channels,\n",
        "                                            output_q_value=num_actions, pool_kernel_size=3,\n",
        "                                            kernel_size=3, dense_layer_features=256,\n",
        "                                            IM_HEIGHT=img_height//2, IM_WIDTH=img_width//2)\n",
        "\n",
        "    buffer = DQN.ReplayBuffer(size_of_buffer=10000) # Experience Replay\n",
        "    batch_size= 32\n",
        "    gamma = 0.99 # Discount factor\n",
        "    num_epochs = 1000\n",
        "    learning_rate = 0.01\n",
        "    # Huber loss to aid small gradients\n",
        "    criterion = F.smooth_l1_loss\n",
        "    n = 10 # Target network parameter update\n",
        "    if use_cuda:\n",
        "        target_dqn_model = target_dqn_model.cuda()\n",
        "        dqn_model = dqn_model.cuda()\n",
        "    model, _ = train(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, num_epochs, criterion, learning_rate,\n",
        "                     use_double_q_learning=True)\n",
        "    # Saving the model\n",
        "    path = '/content/sample_data/Deep-Q-Learning/'\n",
        "    torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}